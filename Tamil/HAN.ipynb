{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d62c4e-1e69-4c6d-915f-d90fe736a1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, LSTM, Bidirectional, TimeDistributed, Dropout, Layer\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31f536c5-77a1-438d-879d-3d59772fc356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_df = pd.read_csv('/Users/arunaa/Python/Sracasam/sarcasm_tam_train.csv')\n",
    "test_df = pd.read_csv('/Users/arunaa/Python/Sracasam/sarcasm_tam_test_without_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66b636a2-18e2-464d-9bb5-55d778a403e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X_train = train_df['Text']\n",
    "y_train = train_df['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff7236e3-7163-401c-9d33-f7670947f20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2faa7a0f-9efe-479b-bec9-5476d5305d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "101ab414-b423-4bc7-8765-7c99f6c375fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data for evaluation\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_padded, y_train_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a6510bb-89c8-4810-ab32-682b83dfa5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Attention layer\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], input_shape[-1]), initializer='uniform', trainable=True)\n",
    "        self.b = self.add_weight(name='att_bias', shape=(input_shape[-1],), initializer='uniform', trainable=True)\n",
    "        self.u = self.add_weight(name='att_var', shape=(input_shape[-1],), initializer='uniform', trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        u_it = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.exp(K.sum(K.dot(u_it, K.expand_dims(self.u, -1)), axis=-1))\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        return K.sum(weighted_input, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08d8a9e2-d668-4240-948b-1ae1ccf26b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the HAN model\n",
    "input_layer = Input(shape=(100,))\n",
    "embedding_layer = Embedding(input_dim=5000, output_dim=128, input_length=100)(input_layer)\n",
    "lstm_layer = Bidirectional(LSTM(units=128, return_sequences=True))(embedding_layer)\n",
    "attention_layer = AttentionLayer()(lstm_layer)\n",
    "dropout_layer = Dropout(0.5)(attention_layer)\n",
    "output_layer = Dense(units=1, activation='sigmoid')(dropout_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6d39360-2a4d-4259-b86c-2e6cdbe3159a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m740/740\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 103ms/step - accuracy: 0.7456 - loss: 0.5325 - val_accuracy: 0.7900 - val_loss: 0.4442\n",
      "Epoch 2/10\n",
      "\u001b[1m740/740\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 126ms/step - accuracy: 0.8264 - loss: 0.3809 - val_accuracy: 0.7940 - val_loss: 0.4400\n",
      "Epoch 3/10\n",
      "\u001b[1m740/740\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 122ms/step - accuracy: 0.8549 - loss: 0.3278 - val_accuracy: 0.7856 - val_loss: 0.4682\n",
      "Epoch 4/10\n",
      "\u001b[1m740/740\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 121ms/step - accuracy: 0.8772 - loss: 0.2871 - val_accuracy: 0.7748 - val_loss: 0.4930\n",
      "Epoch 5/10\n",
      "\u001b[1m740/740\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 121ms/step - accuracy: 0.8934 - loss: 0.2515 - val_accuracy: 0.7775 - val_loss: 0.5804\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history = model.fit(X_train_split, y_train_split, epochs=10, batch_size=32, validation_data=(X_val_split, y_val_split), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f39f0751-e3cb-479b-b97c-6fdb15abac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the test data\n",
    "X_test = test_df['Text']\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "618ca409-953c-4725-895a-f6ad77765c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "test_predictions_prob = model.predict(X_test_padded)\n",
    "test_predictions = (test_predictions_prob > 0.5).astype(int)\n",
    "test_predictions_labels = label_encoder.inverse_transform(test_predictions.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "856aefe7-9ea1-4742-a976-0dd660cbccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to CSV file\n",
    "test_df['Predicted_Labels'] = test_predictions_labels\n",
    "output_path = '/Users/arunaa/Python/Sracasam/predictions_HAN.csv'\n",
    "test_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "138ae49b-06a3-4df6-9f7e-9ba12a491838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 38ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict on the validation set\n",
    "val_predictions_prob = model.predict(X_val_split)\n",
    "val_predictions = (val_predictions_prob > 0.5).astype(int)\n",
    "val_predictions_labels = label_encoder.inverse_transform(val_predictions.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b56977d-2eb0-45cc-bcd6-9c3e31952bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_val_split, val_predictions)\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_val_split, val_predictions, average='weighted')\n",
    "classification_rep = classification_report(y_val_split, val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e04fc39-5a3b-45c7-9807-2ab1cb7ffbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7940480216435577\n",
      "F1 Score: 0.7864257651582147\n",
      "Precision: 0.7841086116420873\n",
      "Recall: 0.7940480216435577\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.86      4318\n",
      "           1       0.65      0.52      0.58      1596\n",
      "\n",
      "    accuracy                           0.79      5914\n",
      "   macro avg       0.74      0.71      0.72      5914\n",
      "weighted avg       0.78      0.79      0.79      5914\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the evaluation metrics and classification report\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e72d273c-224d-4174-8097-ed4a661da19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Text Predicted_Labels\n",
      "0         Kangana wow  awesome yr ye lakdi sbae alh hai    Non-sarcastic\n",
      "1     விழுப்புரம்  வன்னிய கவுண்டர் சார்பாக வாழ்த்துக...    Non-sarcastic\n",
      "2     திரௌபதி திரைப்படம் வெற்றி பெற வாணியர் சமுதாயம்...    Non-sarcastic\n",
      "3     இந்த திரைப்படம் வெற்றிபெற, ஆதி தமிழன் அதாவது இ...    Non-sarcastic\n",
      "4     dai thala pera sonnalay summa tamil naday athi...    Non-sarcastic\n",
      "...                                                 ...              ...\n",
      "6333                      NTR _ Ajith mutuals like here    Non-sarcastic\n",
      "6334  aiyo #thala marana mass #thala love you so muc...        Sarcastic\n",
      "6335                      Yan kadavula I love you thala        Sarcastic\n",
      "6336  Thank you vijay sethupathi....for acted at syr...    Non-sarcastic\n",
      "6337    Amitab and taapsi manu ki copy picture bnai h y    Non-sarcastic\n",
      "\n",
      "[6338 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Print the dataset with text and predicted labels\n",
    "print(test_df[['Text', 'Predicted_Labels']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec79558-19d3-45b2-a402-27d29a897311",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
